ng-template('#slides'='')
    section
        slide-with-header(header="Deep Learning")
            img(src="assets/images/machine-learning/machine-learning-as-subfield-of-ai.png", style={height: "400px"}).mt-5.img-fluid
            div.mt-3.fragment.text-center.alert.alert-primary Deep Learning = Machine Learning mit Neuralen Netzen als Modell
    section
        slide-with-header(header="Deep Learning - Übersicht")
            ul
                li Machine Learning Modell ist ein #[span.highlight Neural Network]
                li Kann Supervised Learning
                li Kann Unsupervised Learning
                li Benötigt #[span.highlight viele Daten]
                li Benötigt #[span.highlight viel Rechenzeit (GPU)]
                li Meistens beste Wahl bei #[span.highlight unstrukturierten Daten] (Sequenzen, Text, Bilder, ...)
                li Bei strukturierte Daten (z.B. ML Lab Tag 1) meistens nicht beste Wahl.
    section
        slide-with-header(header="Deep Learning - Grundidee")
            ul
                li Inspiriert vom biologischen Gehirn
                li Neuronen die mit einander verbunden sind (Knoten in einem Graph).
                li Neuronen haben eingehende und ausgehende Verbindungen (Kanten).
                li Neuronen feuern (geben einen Wert weiter), wenn eingehende Neuronen #[span.highlight genug stark] feuern.
    tables-of-content-fnn
        div.fragment.text-center.alert.alert-warning
            | Feedforward Neural Network =
            br
            | Fully Connected Neural Network =
            br
            | Multi Layer Perceptron (MLP) =
            br
            | Neural Network (NN)
    tables-of-content-fnn([active]="fnnLabels.DATA_SPECIFICATION")
    section
        slide-with-header(header="Neural Network - Specification")
            ul
                li.fragment Was ist das #[span.highlight Ziel]
                li.fragment Was ist die #[span.highlight Kostenfunktion]
                li.fragment Welche #[span.highlight Features] wählen wir
                li.fragment
                    data-specification-element-categorical-feature-encoded
                li.fragment
                    data-specification-element-numerical-feature-standardize
    tables-of-content-fnn([active]="fnnLabels.MODEL")
    section
        slide-with-header(header="Lineare Regression - Als (Neural) Network")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als (Neural) Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="linearRegression")
                .col-6
                    img(src="assets/images/neural-network/linear_regression.png").img-fluid-both
            div.mt-5.fragment.text-center.alert.alert-primary Die Berechnung der Linearen Regression als (Neural) Network dargestellt.
    section
        slide-with-header(header="Hidden Layer(s) hinzufügen")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als (Neural) Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="oneHiddenLayerZ1")
                    div ...
                    div([mathjax]="oneHiddenLayerZh")
                    div.mt-5([mathjax]="oneHiddenLayerOutput")
                .col-6.r-stack
                    img(src="assets/images/neural-network/one_hidden_nn.png").img-fluid-both
                    img.fragment(src="assets/images/neural-network/one_hidden_nn_with_params.png").img-fluid-both
            div.mt-5.fragment.text-center.alert.alert-primary
                div Hidden Layer entspricht mehreren Linearen Regressionen nebeneinander und hintereinander!
    section
        slide-with-header(header="Aktivierungsfunktion hinzufügen")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als Neural Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="oneHiddenLayerWithActivationZ1")
                    div ...
                    div([mathjax]="oneHiddenLayerWithActivationZh")
                    div.mt-5([mathjax]="oneHiddenLayerWithActivationOutput")
                .col-6
                    img(src="assets/images/neural-network/one_hidden_nn_with_phi.png").img-fluid-both
            div.mt-5.fragment.text-center.alert.alert-primary
                div
                    span(class="highlight", mathjax="<math><mi>ϕ</mi></math>")
                    span &nbsp;
                    span nennt man die #[span.highlight Aktivierungsfunktion]
                div.small-font
                    span Die Aktivierungsfunktion macht das Modell #[span.highlight non-linear]. Heute wird oft ReLU verwendet (#[a(href="https://en.wikipedia.org/wiki/Activation_function") Liste von gängigen Aktivierungsfunktionen]).
    section
        slide-with-header(header="Logistic Regression - Als (Neural) Netzwerk")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als (Neural) Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="logisticRegression")
                .col-6
                    img(src="assets/images/neural-network/logistic_regression.png").img-fluid-both
            div.mt-5.fragment.text-center.alert.alert-primary
                div Hier ist die Aktivierungsfunktion der #[span.highlight Sigmoid].
    section
        slide-with-header(header="Neural Network - Mehrere Outputs möglich", [extra]="true")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="oneHiddenLayerWithActivationZ1Colored")
                    div ...
                    div([mathjax]="oneHiddenLayerWithActivationZhColored")
                    div.mt-5([mathjax]="oneHiddenLayerWithActivationY1")
                    div ...
                    div([mathjax]="oneHiddenLayerWithActivationYo")
                .col-6
                    img(src="assets/images/neural-network/one_hidden_nn_with_multiple_outputs.png").img-fluid-both
    section
        slide-with-header(header="Logistic Regression - Mehrere Outputs", [extra]="true")
            .row
                .col-6
                    h5 Als Formell
                .col-6
                    h5 Als Netzwerk
            .row.mt-4
                .col-6.small-font
                    div([mathjax]="logisticRegressionY1")
                    div ...
                    div([mathjax]="logisticRegressionYo")
                .col-6
                    img(src="assets/images/neural-network/logistic_regression_with_multiple_outputs.png").img-fluid-both
            div.mt-5.fragment.text-center.alert.alert-primary
                div Logistische Regression mit mehreren Outputs.
                div Hier ist die Aktivierungsfunktion #[span.highlight Softmax].

    section
        slide-with-header(header="Deep Learning - Framework für Modelle")
            ul(style={fontSize: "32px"})
                li Modellkomplexität einfach anpassbar
                ul
                    li #[span.highlight mehr Hidden Layers] => #[span.highlight Mehr lernbare Parameter]
                    li #[span.highlight weniger Hidden Layers] => #[span.highlight Weniger lernbare Parameter]
                    li In einem Layer #[span.highlight mehr Nodes] => #[span.highlight Mehr lernbare Parameter]
                    li In einem Layer #[span.highlight weniger Nodes] => #[span.highlight Weniger lernbare Parameter]
                li.mt-2 Kostenfunktion
                ul
                    li Kostenfunktion muss #[span.highlight ableitbar] sein.
                    li Kann Problem spezifisch gewählt werden.
                li.mt-2 Viele weitere Möglichkeiten (Deep Learning)
                ul
                    li Wir können die Architektur (Verbindungen) vom Netzwerk anders gestalten, entsprechend dem zugrunde liegenden Problem (z.B. CNN und RNN).
    section
        slide-with-header(header="Deep Learning - Gelerntes Feature Engineering")
           .row
               .col-12
                   img(src="assets/images/neural-network/one_hidden_nn_with_phi.png", style={height: "350px", marginLeft: "100px"}).img-fluid-both
               .col-12.mt-5.scale-05
                   model-3-visualization(input-header="Input Space", [with-trigger]="false", feature-header="Feature Space", output-header="Output Space" )
                       div(input).rect-styling
                           div(mathjax="x")
                       div(pre).model-box Feature Engineering
                       div(feature).rect-styling
                           div(mathjax="z")
                       div(model).model-box Lineares Modell
                       div(output).rect-styling
                           div(mathjax="y")
           div.fragment.text-center.alert.alert-primary.small-font
               | Die Hidden Layers können als #[span.highlight lernbares Feature Engineering] betrachtet werden.
               | Die gelernten Features haben gewisse #[span.highlight praktische Einschränkungen].
    section
        slide-with-header(header="Deep Learning - Gelernte Dimensionality Reduction")
            div.primary
            .row
                .col-12
                    img(src="assets/images/neural-network/one_hidden_nn_with_phi.png", style={height: "350px", marginLeft: "100px"}).img-fluid-both
                .col-12.mt-5.scale-05
                    model-3-visualization("#encMod"="", [with-trigger]="false", output-header="Output Space" )
                        div(inputHeader)
                            div(style={marginTop: "-75px"})
                                h5(style={marginBottom: "0"}) Input Space
                                h6(margin="0") 3072 Features
                        div(featureHeader)
                            div(style={marginTop: "-50px"})
                                h5(style={marginBottom: "0"}) Latent Space
                                h6(margin="0") 200 Features
                        div(input).rect-styling
                            img(src="assets/images/pca/example-img.png")
                        div(pre).model-box
                            div Encoder
                        div(feature).fix-height.code
                            | [ -57.2, -3.6, ..., 2.3, -3.4 ]
                        div(model).model-box
                            div Lineares Modell
                        div(output).rect-styling
                            div.code Deer
            div.fragment.text-center.alert.alert-primary.small-font
                | Die Hidden Layers können als #[span.highlight lernbare Dimensionality Reduction] betrachtet werden.
                | Dabei lernt das Modell, welche Informationen für #[span.highlight das spezifische Ziel] weggeworfen werden können.
    tables-of-content-fnn([active]="fnnLabels.COST_FUNCTION")
    section
        slide-with-header(header="Problemspezifische Kosten Funktion", [extra]="true")
            ul
                li Bei Regression: MSE / MAE
                li Bei Klassifikation: Maximum Liklihood
                li Bei Unsupervised: Reconstruction Error
                li ...
    tables-of-content-fnn([active]="fnnLabels.OPTIMIERUNG")
    section
        slide-with-header(header="Gradient Descent")
            .row
                .col-6
                    h5 Reminder: Convex Problem (Tag 1)
                    div.small-font Bei Linearen Modellen
                    img.mt-5(src="assets/images/gradient-descent/gradient_descent_step_5.png")
                .col-6
                    h5 Non-Convex Problem
                    div.small-font Bei NN mit Hidden Layer(s)
                    img.mt-5(src="assets/images/neural-network/gd_nn.png").img-fluid-both
            div.mt-4.fragment.text-center.alert.alert-primary
                | Gleicher Algorithmus. Aber bei non-convex Kostenfunktionen, können wir in einem #[span.highlight lokalen Minimum] landen.
    section
        slide-with-header(header="Stochastic und Batch Gradient Descent")
            ul
                li Wie funktionierte Gradient Descent? (Tafel)
                li.fragment #[span.highlight Stochastic Gradient Descent] berechnet die Richtung zum Minimum mit nur einem Sample.
                    ul.fragment
                        li Update viel schneller zu berechnen.
                        li Update sehr noisy (oft in falsche Richtung)
                li.fragment #[span.highlight Batch Gradient Descent] berechnet die Richtung zum Minimum mit z.B. 128 Sample (Batch Size).
                    ul.fragment
                        li Update schneller zu berechnen.
                        li Update noisy (oft in falsche Richtung)
            div.mt-4.fragment.text-center.alert.alert-warning(style={fontSize: "26px"})
                | In Literatur heisst #[span.highlight Batch Gradient Descent] oft auch #[span.highlight Stochastic Gradient Descent].
    section
        slide-with-header(header="Backpropagation", [extra]="true")
            ul(style={fontSize: "22px"})
                li Algorithmus für das #[span.highlight effiziente Berechnen der Gradienten] der Kostenfunktion #[span.highlight in einem Neural Network]
                li Nutzt die Chain-Rule von der Analysis um Zwischenergebnisse zu cachen.
            .r-stack.mt-5
                .row(style={background: "white", width: "100%"}).fragment
                    .col-6
                        img(src="assets/images/neural-network/very_simple_nn_with_params.png").img-fluid-both
                    .col-6
                        div([mathjax]="backpropSetting", style={fontSize: "22px"})
                .row(style={background: "white", width: "100%"}).fragment
                    .col-6
                        img(src="assets/images/neural-network/very_simple_nn.png").img-fluid-both
                    .col-6
                        div([mathjax]="backpropSetting2", style={fontSize: "22px"})
                .row(style={background: "white", width: "100%"}).fragment
                    .col-6
                        img(src="assets/images/neural-network/very_simple_nn.png").img-fluid-both
                    .col-6
                        div([mathjax]="backpropSetting3", style={fontSize: "22px"})
            div.fragment.text-center.alert.alert-warning(style={fontSize: "22px"})
                div Optimierungsalgorithmus von Neuralen Netzen ist (Batch) Gradient Descent.
                div #[span.highlight Backpropagation] hilft "nur" beim #[span.highlight effizienten Berechnen der Gradienten].
    section
        slide-with-header(header="[Additional Resources]", [extra]="true")
            ul
                li Neuronen anders vernetzen
                    ul
                        li ResNet (skip connection)
                        li Convolutional Neural Network (CNN)
                        li Recurrent Neural Network (RNN)
                        li Transformer (behandelt im NLP)
                li Learning und Regularization: Momentum, Adam, Learning Rate Decay, Dropout, Batch Normalization
            div.fragment.text-center.alert.alert-primary.mt-2
                 | Grundidee immer: #[span.highlight Annahmen treffen], um das Lernen zu vereinfachen ohne es (stark) einzuschränken.
